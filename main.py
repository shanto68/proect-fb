import os
import json
import feedparser
import requests
import google.generativeai as genai
from newspaper import Article
from utils import check_duplicate, download_image, post_fb_comment, extract_og_image

# -----------------------------
# 1Ô∏è‚É£ Configuration
# -----------------------------
RSS_FEED = os.environ.get("RSS_FEED_URL")
FB_PAGE_ID = os.environ.get("FB_PAGE_ID")
FB_ACCESS_TOKEN = os.environ.get("FB_ACCESS_TOKEN")
GEN_API_KEY = os.environ.get("GEMINI_API_KEY")
LOG_FILE = "posted_articles.json"

if not RSS_FEED:
    print("‚ùå RSS_FEED_URL not provided.")
    exit()

genai.configure(api_key=GEN_API_KEY)

# -----------------------------
# 2Ô∏è‚É£ Load posted articles
# -----------------------------
try:
    with open(LOG_FILE, "r") as f:
        posted_articles = json.load(f)
except:
    posted_articles = []

# -----------------------------
# 3Ô∏è‚É£ Fetch latest RSS entry
# -----------------------------
feed = feedparser.parse(RSS_FEED)
if not feed.entries:
    print("‚ùå No RSS entries found.")
    exit()

first_entry = feed.entries[0]
title = first_entry.title
article_url = first_entry.link

print("üì∞ Latest Article:", title)
print("üîó URL:", article_url)

# -----------------------------
# 4Ô∏è‚É£ Duplicate check
# -----------------------------
if title in posted_articles or check_duplicate(title):
    print("‚ö†Ô∏è Already posted or duplicate. Skipping.")
    exit()

# -----------------------------
# 5Ô∏è‚É£ Extract full content
# -----------------------------
try:
    article = Article(article_url, language="bn")  # Bengali support
    article.download()
    article.parse()
    full_content = article.text
except Exception as e:
    print("‚ùå Full content extraction failed:", e)
    full_content = title

# -----------------------------
# 6Ô∏è‚É£ Featured image (RSS or og:image fallback)
# -----------------------------
featured_image = None
if hasattr(first_entry, "media_content"):
    for media in first_entry.media_content:
        if media.get("url"):
            featured_image = media["url"]
            break

# Fallback to newspaper top_image
if not featured_image and getattr(article, "top_image", None):
    featured_image = article.top_image

# Fallback to og:image
if not featured_image:
    featured_image = extract_og_image(article_url)

local_image = None
if featured_image:
    filename = "featured.jpg"
    if download_image(featured_image, filename):
        local_image = filename
        print("‚úÖ Featured image downloaded:", filename)
    else:
        print("‚ùå Failed to download featured image")
else:
    print("‚ö†Ô∏è No featured image found")

# -----------------------------
# 7Ô∏è‚É£ Generate hashtags only (Gemini AI)
# -----------------------------
model = genai.GenerativeModel("gemini-2.5-flash")

hashtag_prompt = f"""
Generate 3-5 relevant Bengali hashtags for this news article.
Title: {title}
Full Content: {full_content}
"""
hashtag_resp = model.generate_text(hashtag_prompt)
hashtags = [tag.strip() for tag in hashtag_resp.split() if tag.startswith("#")]
hashtags_text = " ".join(hashtags)

# -----------------------------
# 8Ô∏è‚É£ Prepare FB post content
# -----------------------------
fb_content = f"üì∞ Original News: {title}\n\n{full_content}\n\nüîó Source: {article_url}\n\n{hashtags_text}"
print("‚úÖ FB Post Content Ready")

# -----------------------------
# 9Ô∏è‚É£ Post to Facebook
# -----------------------------
fb_result = []

if local_image:
    with open(local_image, "rb") as f:
        data = {"caption": fb_content, "access_token": FB_ACCESS_TOKEN}
        files = {"source": f}
        r = requests.post(f"https://graph.facebook.com/v17.0/{FB_PAGE_ID}/photos", data=data, files=files)
        fb_result.append(r.json())
else:
    post_data = {"message": fb_content, "access_token": FB_ACCESS_TOKEN}
    r = requests.post(f"https://graph.facebook.com/v17.0/{FB_PAGE_ID}/feed", data=post_data)
    fb_result.append(r.json())

print("üì§ Facebook Response:", fb_result)

# -----------------------------
# üîü Log successful post
# -----------------------------
posted_articles.append(title)
with open(LOG_FILE, "w") as f:
    json.dump(posted_articles, f)
